{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chapter8.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOzs7nYf05/UPznHrSrqRLQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"KYJk6EgarSV5"},"source":["#建立一個function針對不同的temperature設定，對機率分布重新加權並計算出新的機率分布\n","import numpy as np\n","\n","def reweight_distribution(original_distribution,temperature=0.5):\n","  distribution = np.log(original_distribution)/temperature\n","  distribution = np.exp(distribution)\n","  return distribution / np.sum(distribution) #傳回重新加權"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ULzkEoBArcu0"},"source":["ori_dstri = np.array([0.8,0.1,0.1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I2OA0X1SreJE"},"source":["new_dstri = reweight_distribution(ori_dstri,temperature=0.01)\n","print(new_dstri)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HlSuBwmbrfML"},"source":["new_dstri = reweight_distribution(ori_dstri,temperature=2)\n","print(new_dstri)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iAWGQW6FrgVT"},"source":["new_dstri = reweight_distribution(ori_dstri,temperature=10)\n","print(new_dstri)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MSjLlx3zriTN"},"source":["#實現字元級LSTM文字資料生成 (本機端上操作)\n","import keras\n","import numpy as np\n","\n","path = keras.utils.get_file(\n","    'nietzsche.txt',\n","    origin = 'https://s3.amazonaws.com/text-datasets/nietzsche.txt'\n",")\n","text = open(path).read().lower()\n","print('Corpus length:',len(text))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nAGu6rPFrlmd"},"source":["#向量化字元序列\n","maxlen = 60\n","step = 3\n","sentences = []\n","next_chars = []\n","\n","for i in range(0,len(text)-maxlen,step):\n","  sentences.append(text[i:i+maxlen])\n","  next_chars.append(text[i+maxlen])\n","print('Number of sequences:',len(sentences))\n","\n","chars = sorted(list(set(text)))\n","print('Unique charactors:',len(chars))\n","\n","# 將各個字元對應到\"chars\"串列中的索引值，成為字典格式\n","char_indices = dict((char,char.index(char)) for char in chars)\n","\n","# 將字元經One-Hot編碼為二元陣列\n","print('Vectorization....')\n","x = np.zeros((len(sentences),maxlen,len(chars)),dtype=np.bool)\n","y = np.zeros((len(sentences),len(chars)),dtype=np.bool)\n","\n","for i,sentence in enumerate(sentences):\n","  for t , char in enumerate(chars):\n","    x[i,t,char_indices[char]] = 1\n","  y[i,char_indices[next_chars[i]]] = 1\n","\n","print(x.shape)\n","print(y.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jXQjn9Sfrrpn"},"source":["#建立神經網路\n","from keras import layers \n","\n","model = keras.models.Sequential()\n","model.add(layers.LSTM(128,input_shape=(maxlen,len(chars))))\n","model.add(layers.Dense(len(chars),activation='softmax')) # 預測字元種類，共57種"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QYwuQNW8rtYO"},"source":["#模型編譯設定\n","optimizer = keras.optimizers.RMSprop(lr=0.01)\n","model.compile(loss='categorical_crossentropy',optimizer=optimizer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OHkr2Lv1rxUH"},"source":["#訓練語言模型並從中取樣\n","def sample(preds,temperature=1.0):\n","  preds = np.asarray(preds).astype('float64')\n","  # ======== 重新加權計算熵 ===============\n","  preds = np.log(preds) / temperature\n","  exp_preds = np.exp(preds)\n","  preds = exp_preds / np.sum(exp_preds)\n","  \n","  probas = np.random.multinomial(1,preds,1)\n","  return np.argmax(probas) # 取出多項式分佈的結果"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CQluh2_Pr0Uf"},"source":["#循環文字的生成\n","import random \n","import sys\n","\n","for epoch in range(1,60):\n","  print('epoch',epoch)\n","  model.fit(x,y,batch_size=128,epochs=1)\n","\n","  # 隨機選擇文本中的某段60個字元的文字 (初始文字)\n","  start_index = random.randint(0,len(text)-maxlen-1)\n","  generated_text = text[start_index:start_index+maxlen]\n","  \n","  print('-----random initial text:\"',generated_text,'\"')\n","\n","\n","  for temperature in [0.2,0.5,1.0,1.2]:\n","    print('-----temperature:',temperature)\n","    sys.stdout.write(generated_text)\n","\n","    # 每個temperature生成400個字元\n","    for i in range(400):\n","      sampled = np.zeros((1,maxlen,len(chars)))\n","      for t,char in enumerate(generated_text):\n","        sampled[0,t,char_indices[char]] = 1.\n","      \n","      preds = model.predict(sampled,verbose=0)[0]\n","      next_index = sample(preds,temperature)\n","      next_char = chars[next_index]\n","      generated_text += next_char\n","      generated_text = generated_text[1:]\n","      sys.stdout.write(next_char)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SU5_Ch_Qr3kn"},"source":["#DeepDream\n","#載入預先訓練的Inception V3 模型\n","from tensorflow.keras.applications import inception_v3\n","from tensorflow.keras import backend as K\n","import tensorflow as tf\n","tf.compat.v1.disable_eager_execution()\n","\n","K.set_learning_phase(0) # 終止所有訓練相關的操作\n","model = inception_v3.InceptionV3(weights='imagenet',include_top=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-6SIalZHr-fA"},"source":["#計算損失\n","layer_contributions={\n","  'mixed2':0.2,\n","  'mixed3':2.,  \n","  'mixed4':2.,\n","  'mixed5':1.5,\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fvg49fYosAMT"},"source":["#定義損失最大化\n","layer_dict = dict([(layer.name,layer) for layer in model.layers])\n","\n","loss = K.variable(0.)\n","for layer_name in layer_contributions:\n","  coeff = layer_contributions[layer_name]\n","  activation = layer_dict[layer_name].output\n","  scaling = K.prod(K.cast(K.shape(activation),'float32'))\n","\n","  loss = loss + coeff * K.sum(K.square(activation[:,2:-2,2:-2,:])) / scaling"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cAE88e5HsCkx"},"source":["#梯度上升處理程序\n","dream = model.input\n","print(dream.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i6Iz0Hz3sFRJ"},"source":["\n","grads = K.gradients(loss,dream)[0]\n","grads /= K.maximum(K.mean(K.abs(grads)),1e-7) # 正規化梯度\n","\n","# 在給定輸入圖片的情況下，自定一個Keras已取得損失值與梯度值\n","outputs = [loss,grads]\n","fetch_loss_and_grads = K.function([dream],outputs)\n","\n","def eval_loss_and_grads(x):\n","  outs = fetch_loss_and_grads([x])\n","  loss_value = outs[0]\n","  grad_value = outs[1]\n","  return loss_value,grad_value\n","\n","def gradient_ascent(x,iterations,step,max_loss=None):\n","  loss_value,grad_values = eval_loss_and_grads(x)\n","  for i in range(iterations):\n","    if loss_value is not None and loss_value > max_loss:\n","      break\n","    print('...Loss value ar',i,':',loss_value)\n","    print('...grad value ar',i,':',grad_values)\n","    x += step*grad_values\n","  return"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GFeI_35gsJJ5"},"source":["#DeepDream 演算法\n","import scipy\n","from keras.preprocessing import image \n","import numpy as np\n","\n","\n","# 對葡片進行預處理\n","def preprocess_image(image_path):\n","  img = image.load_img(image_path)\n","  img = image.img_to_array(img)\n","  print(img.shape)\n","  img = np.expand_dims(img,axis=0)\n","  print(img.shape)\n","  img = inception_v3.preprocess_input(img)\n","  return img\n","\n","# 將Inception V3所做的預處理進行反向操作，轉回圖片格式\n","def deprocess_image(x):\n","  if K.image_data_format()=='channels_first':\n","    x = x.reshape((3,x.shape[2],x.shape[3]))\n","    x = x.transpose((1,2,0))\n","  else:\n","    x = x.reshape((x,shape[1],x.shape[2],3))\n","  x /= 2.\n","  x *= 0.5\n","  x *= 255.\n","  x = np.clip(x,0,255).astype('unit8') # 將數字限制在0-255之間\n","  return x\n","\n","# 進行圖片比例的縮放\n","def resize_img(img,size):\n","  img = np.copy(img)\n","  factors = (1,\n","        float(size[0])/img.shape[1],\n","        float(size[1])/img.shape[2],\n","        1\n","        )\n","  return scipy.ndimage.zoom(img,factors,order=1) #以樣條插值法的技術對圖片進行縮放\n","\n","\n","# 儲存圖片，於儲存前反轉Inception V3所做的預處理\n","def save_img(img,fname):\n","  pil_img = deprocess_image(np.copy(img))\n","  scipy.misc.imsave(fname,pil_img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XQt4D00PsLF5"},"source":["#在不同的連續比例的圖片上執行梯度上升\n","step = 0.01\n","num_octave = 3\n","octave_scale = 1.4\n","iterations = 20\n","max_loss = 10.\n","\n","base_image_path = 'original_photo_deep_dream.png'\n","img = preprocess_image(base_image_path) # 載入圖片並進行預處理\n","original_shape = img.shape[1:3]\n","successive_shapes = [original_shape]\n","\n","for i in range(1,num_octave):\n","  shape = tuple([int(dim / (octave_scale ** i)) for dim in original_shape])\n","  successive_shapes.append(shape)\n","successive_shapes = successive_shapes[::-1] # 反轉寬高比例list，使他們按照大小順序遞增\n","\n","original_img = np.copy(img)\n","shrunk_original_img = resize_img(img,successive_shapes[0])\n","\n","for shape in successive_shapes: # 開始逐次放大圖片\n","  print('Preprocessing image shape',shape)\n","  img = resize_img(img,shape)\n","  img = gradient_ascent(img,iterations=iterations,step=step,max_loss=max_loss) # 執行梯度上升\n","  upscaled_shrunk_original_img = resize_img(shrunk_original_img,shape) # 將小比例圖片放大至目前比例，易造成像素顆粒化(小→大)\n","\n","  same_size_original = resize_img(original_img,shape) # 將原始圖片縮小至目前比例(大→小)\n","  lost_detail = same_size_original - upscaled_shrunk_original_img # 相減求得損失的細節\n","  img += lost_detail # 將損失的細節放回圖片中\n","  shrunk_original_img = resize_img(original_img,shape)\n","  save_img(img,frame='dream_at_scale_'+str(shape)+'.png')\n","\n","save_img(img,fname='final_dream.png')"],"execution_count":null,"outputs":[]}]}