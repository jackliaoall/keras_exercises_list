{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chapter6.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPzucRKZnYYLrbgoyZgSCXK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"h9M2lx6fo2dm"},"source":["#one-hot encoding\n","import numpy as np\n","\n","samples = ['The cat sat on the mat.','The dog eat my homework.']\n","\n","token_index = {}\n","\n","# 建立字典\n","for sample in samples:\n","  for word in sample.split():\n","    if word not in token_index:\n","      token_index[word]=len(token_index)+1\n","\n","# 字典建立完成\n","\n","max_length = 10\n","\n","results = np.zeros(shape=(len(samples),max_length,max(token_index.values())+1))\n","\n","print(results.shape)\n","\n","for i,sample in enumerate(smaples):\n","  for j , word in list(enumerate(sample.split()))[:max_length]:\n","    index = token_index.get(word)\n","    results[i,j,index] = 1. # 把對應元素設為1\n","\n","results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OOY1d2BcpA7s"},"source":["#字元的one-hot encoding\n","import string\n","\n","samples = ['The cat sat on the mat.','The dog eat my homework.']\n","characters = string.printable\n","print(len(characters))\n","\n","token_index = dict(zip(characters,range(1,len(characters)+1)))\n","\n","# 字典建立完成\n","\n","max_length = 50\n","\n","results = np.zeros(shape=(len(samples),max_length,max(token_index.values())+1))\n","\n","print(results.shape)\n","\n","for i,sample in enumerate(smaples):\n","  for j , character in enumerate(sample):\n","    index = token_index.get(character)\n","    results[i,j,index] = 1. # 把對應元素設為1\n","\n","results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v8bHY_3JpIlC"},"source":["#用Keras的內建工具 做 one-hot encoding\n","from keras.preprocessing.text import Tokenizer\n","\n","samples = ['The cat sat on the mat.','The dog eat my homework.']\n","\n","tokenizer = Tokenizer(num_words=1000) # 建立分類器，處理前1000個最常用的單字\n","tokenizer.fit_on_texts(samples)\n","sequences = tokenizer.texts_to_sequences(samples)\n","\n","print(sequences)\n","\n","one_hot_results = tokenizer.texts_to_matrix(samples,mode='binary')\n","\n","print(one_hot_results.shape)\n","print(one_hot_results[0][:15]) # 第一個樣本的前15個\n","print(one_hot_results[1][:15]) # 第二個樣本的前15個\n","\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens'% len(word_index))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SgZnB7t9pPoa"},"source":["#使用雜湊技巧做 one-hot encoding\n","samples = ['The cat sat on the mat.','The dog eat my homework.']\n","\n","dimensionality = 1000\n","max_length = 10\n","\n","results = np.zeros((len(samples),max_length,dimensionality))\n","\n","for i ,smaple in enumerate(samples):\n","  for j,eord in list(enumerate(sample.split()))[:max_length]:\n","    index = abs(hash(word))%dimensionality # 將token雜湊成0到1000之間的隨機整數索引\n","    results[i,j,index] = 1.\n","\n","print(results.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cx61wvdmpTNY"},"source":["#用Keras 的 Embeddung Layer來實作文字崁入法\n","from keras.layers import Embedding\n","embedding_layer = Embedding(1000,64)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x3wakCu1pZSQ"},"source":["from keras.datasets import imdb\n","from keras import preprocessing\n","\n","max_features = 10000\n","maxlen = 20\n","\n","(x_train,y_train),(x_test,y_test) = imdb.load_data(num_words=max_features) # 將文字評論以整數(鍵值)list載入\n","\n","print(x_train.shape)\n","x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=maxlen)\n","print(x_train.shape)\n","\n","print(x_train[0])\n","\n","x_test = preprocessing.sequence.pad_sequences(x_test,maxlen = maxlen)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dW-s7sW-pc2g"},"source":["#訓練模型\n","from keras.models import Sequential\n","from keras.layers import Flatten,Dense,Embedding\n","\n","model = Sequential()\n","model.add(Embedding(10000,8,input_length=maxlen))\n","model.add(Flatten())\n","model.add(Dense(1,activation='sigmoid'))\n","model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n","model.summary()\n","\n","history = model.fit(x_train,y_train,epochs=10,batch_size=32,validation_split=0.2)\n","# 驗證準確度: 約 74.66%\n","print(history.history['val_acc'][-1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9bWQiXdApjnX"},"source":["#自原始文字資料到文字崁入向量\n","import os\n","\n","imdb_dir = './data/aclImdb'\n","train_dir=os.path.join(imdb_dir,'train')\n","\n","labels=[]\n","texts=[]\n","\n","for label_type in ['neg','pos']:\n","    dir_name = os.path.join(train_dir,label_type)\n","    for fname in os.listdir(dir_name):\n","        if fname[-4:] == '.txt':\n","            f = open(os.path.join(dir_name,fname),encoding='utf8')\n","            texts.append(f.read())\n","            f.close()\n","            if label_type == 'neg':\n","                labels.append(0)\n","            else:\n","                labels.append(1)\n","print(len(texts))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-trmT9sJpl-X"},"source":["#對資料進行向量化\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","import numpy as np \n","\n","maxlen = 100\n","training_smaples = 200\n","validation_samples = 10000\n","max_words = 10000\n","\n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(texts)\n","sequences = tokenizer.texts_to_sequences(texts)\n","\n","word_index = tokenizer.word_index\n","\n","print('共使用了 %s 個token字詞' %len(word_index))\n","\n","data = pad_sequences(sequences,maxlen=maxlen)\n","labels = np.asarray(labels)\n","\n","print('資料張量 shape:',data.shape)\n","print('標籤張量 shape:',labels.shape)\n","\n","indices = np.arange(data.shape[0])\n","np.random.shuffle(indices)\n","data = data[indices]\n","labels = labels[indices]\n","\n","x_train = data[:training_smaples]\n","y_train = labels[:training_smaples]\n","x_val = data[training_smaples:training_smaples+validation_samples]\n","y_val = labels[training_smaples:training_smaples+validation_samples]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kwp-WKMgppyP"},"source":["#下載 GLOVE 文字崁入向量\n","#使用其中的100維數值向量做為所需要的資料\n","\n","glove_dir = './data'\n","\n","embeddings_index ={}\n","f = open(os.path.join(glove_dir,'glove.6B.100d.txt'),encoding='UTF-8')\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:],dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","print('共使用了 %s 個文字崁入向量' % len(embeddings_index))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6c5YASxdpuQ3"},"source":["#準備 GloVe 文字嵌入向量矩陣\n","embedding_dim= 100\n","\n","embedding_matrix = np.zeros((max_words,embedding_dim))\n","\n","for word,i in word_index.items():\n","    if i < max_words:\n","        embedding_vector = embeddings_index.get(word)\n","        if embedding_matrix[i] is not None:\n","            embedding_matrix[i] = embedding_vector"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mvb7yOwYpxSl"},"source":["#模型定義\n","from keras.models import Sequential\n","from keras.layers import Embedding, Flatten, Dense\n","\n","model = Sequential()\n","# 參數 樣本數, 嵌入向量維度, \n","model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n","model.add(Flatten())\n","model.add(Dense(32, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SvVgj-Q6p1Xl"},"source":["model.layers[0].set_weights([embedding_matrix])\n","model.layers[0].trainable = False # 凍結崁入層\n","\n","model.compile(optimizer='rmsprop',\n","              loss='binary_crossentropy',\n","              metrics=['acc'])\n","history = model.fit(x_train, y_train,\n","                    epochs=10,\n","                    batch_size=32,\n","                    validation_data=(x_val, y_val))\n","model.save_weights('./model/pre_trained_glove_model.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TrxpGC4rp5EV"},"source":["#繪製結果\n","\n","import matplotlib.pyplot as plt \n","\n","acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(1 , len(acc)+1)\n","\n","plt.plot(epochs,acc,'bo',label='Training acc')\n","plt.plot(epochs,val_acc ,'b',label = 'Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","\n","plt.figure()\n","\n","plt.plot(epochs,loss,'bo',label='Training loss')\n","plt.plot(epochs,val_loss ,'b',label = 'Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rfxNua9Ip8IE"},"source":["#訓練相同模型而不使用預先訓練的文字嵌入向量\n","from keras.models import Sequential\n","from keras.layers import Embedding, Flatten, Dense\n","\n","model = Sequential()\n","model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n","model.add(Flatten())\n","model.add(Dense(32, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","model.summary()\n","\n","model.compile(optimizer='rmsprop',\n","              loss='binary_crossentropy',\n","              metrics=['acc'])\n","\n","history = model.fit(x_train, y_train,\n","                    epochs=10,\n","                    batch_size=32,\n","                    validation_data=(x_val, y_val))\n","\n","\n","############### 繪製\n","import matplotlib.pyplot as plt\n","\n","acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(1, len(acc) + 1)\n","\n","plt.plot(epochs, acc, 'bo', label='Training acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R7PYcS_zp_7k"},"source":["test_dir = os.path.join(imdb_dir, 'test')\n","\n","labels = []\n","texts = []\n","\n","for label_type in ['neg', 'pos']:\n","\tdir_name = os.path.join(test_dir, label_type)\n","\tfor fname in sorted(os.listdir(dir_name)):\n","\t\tif fname[-4:] == '.txt':\n","\t\t\tf = open(os.path.join(dir_name, fname), encoding='UTF-8')\n","\t\t\ttexts.append(f.read())\n","\t\t\tf.close()\n","\t\t\tif label_type == 'neg':\n","\t\t\t\tlabels.append(0)\n","\t\t\telse:\n","\t\t\t\tlabels.append(1)\n","\n","sequences = tokenizer.texts_to_sequences(texts)\n","x_test = pad_sequences(sequences, maxlen=maxlen)\n","y_test = np.asarray(labels)\n","\n","model.load_weights('./model/pre_trained_glove_model.h5')\n","model.evaluate(x_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_xX7D3CnqB4E"},"source":["#6-2 循環神經網路 RNN\n","#以 Numpy 實現簡單的 RNN\n","#概念：將輸入和狀態轉換成輸出→參數化兩個權重矩陣\n","import numpy as np\n","\n","timesteps = 100   # 輸入序列資料中的時間點數量\n","input_features = 32  # 輸入特徵空間的維度數\n","output_features = 64  # 輸出特徵空間的維度數\n","\n","inputs = np.random.random((timesteps, input_features))  # 輸入資料：隨機產生數值以便示範\n","\n","state_t = np.zeros((output_features, ))  # 初始狀態：全零向量\n","\n","W = np.random.random((output_features, input_features))  # 建立隨機權重矩陣\n","U = np.random.random((output_features, output_features))\n","b = np.random.random((output_features, ))\n","\n","successive_outputs = []\n","for input_t in inputs:  #  input_t 是個向量, shape 為 (input_features, )\n","    output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)  # 結合輸入與當前狀態(前一個輸出)以取得當前輸出\n","    successive_outputs.append(output_t)  # 將此輸出儲存在列表中\n","    state_t = output_t  #更新下一個時間點的網絡狀態\n","\n","final_output_sequence = np.concatenate(successive_outputs, axis=0)  \n","print(final_output_sequence.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oKHIpqVgqI-0"},"source":["#透過Keras的SimpleRNN層來進行實作\n","from keras.models import Sequential\n","from keras.layers import Embedding , SimpleRNN\n","\n","model = Sequential()\n","model.add(Embedding(10000,32))\n","model.add(SimpleRNN(32))\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q765ZJjxqNgE"},"source":["model = Sequential()\n","model.add(Embedding(10000,32))\n","model.add(SimpleRNN(32,return_sequences=True)) # 增加代表時間點的軸\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pv8kCC8zqPBS"},"source":["#增加多個循環層可以增加神經網路的表現能力\n","model = Sequential()\n","model.add(Embedding(10000,32))\n","model.add(SimpleRNN(32,return_sequences=True)) \n","model.add(SimpleRNN(32,return_sequences=True)) \n","model.add(SimpleRNN(32,return_sequences=True)) \n","model.add(SimpleRNN(32))\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JD9SrQOzqQ3L"},"source":["#準備 IMDB 資料\n","from keras.datasets import imdb\n","from keras.preprocessing import sequence\n","\n","max_features = 10000  #考慮做為特徵的文字數量\n","maxlen = 500  # 我們只看每篇評論的前 500 個文字\n","\n","batch_size = 32\n","\n","print('讀取資料...')\n","(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n","print(len(input_train), 'train sequences') # 25000 筆訓練用序列資料 (評論)\n","print(len(input_test), 'test sequences')\t# 25000 筆測試用序列資料\n","\n","print('Pad sequences (samples x time)')\n","input_train = sequence.pad_sequences(input_train, maxlen=maxlen) # 1. 只看每篇評論的前 500 個文字, 多的去除, 不足填補\n","input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n","print('input_train shape:', input_train.shape)\t# shape=(25000, 500)\n","print('input_test shape:', input_test.shape)\t# shape=(25000, 500)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cecf86e7qTrT"},"source":["#以嵌入向量 Embedding 層和 SimpleRNN 層訓練模型\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Embedding, SimpleRNN\n","\n","model = Sequential()\n","model.add(Embedding(max_features, 32))\n","model.add(SimpleRNN(32))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n","history = model.fit(input_train, y_train,\n","                    epochs=10,\n","                    batch_size=128,\n","                    validation_split=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E5a_yjJpqYD6"},"source":["#繪製結果\n","import matplotlib.pyplot as plt\n","\n","acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(1, len(acc) + 1)\n","\n","plt.plot(epochs,acc,'bo',label='Training acc')\n","plt.plot(epochs,val_acc ,'b',label = 'Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","\n","plt.figure()\n","\n","plt.plot(epochs,loss,'bo',label='Training loss')\n","plt.plot(epochs,val_loss ,'b',label = 'Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CmVlDm9wqcr0"},"source":["#長期短期記憶(Long Short-Term Memory,LSTM)\n","#在 Keras 中使用 LSTM 層\n","from keras.layers import LSTM\n","\n","model = Sequential()\n","model.add(Embedding(max_features, 32))\n","model.add(LSTM(32))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer='rmsprop',\n","loss='binary_crossentropy',\n","metrics=['acc'])\n","history = model.fit(input_train, y_train,\n","                    epochs=10,\n","                    batch_size=128,\n","                    validation_split=0.2)\n","\n","import matplotlib.pyplot as plt\n","\n","acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(1, len(acc) + 1)\n","\n","plt.plot(epochs, acc, 'bo', label='Training acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation acc')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2uTAbbW0qh0K"},"source":["#6-3 循環神經網路的進階用法¶\n","#循環丟棄 (Recurrent dropout)\n","#堆疊循環層 (Stacking recurrent layers)\n","#雙向循環層 (Bidirectional recurent layers)\n","\n","#檢視耶拿天氣資料集的資料\n","import os\n","\n","data_dir = r'./data/'  # 您的 jena_climate 資料夾路徑\n","fname = os.path.join(data_dir, 'jena_climate_2009_2016.csv') # 資料集完整路徑\n","\n","f = open(fname)\n","data = f.read()\n","f.close()\n","\n","lines = data.split('\\n')\n","header = lines[0].split(',')\n","lines = lines[1:]\n","\n","print(header)  # \n","print(len(header))\n","print(len(lines)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yYxSTEvWqnsU"},"source":["#解析資料\n","import numpy as np\n","\n","float_data = np.zeros((len(lines), len(header) - 1))\n","for i, line in enumerate(lines):\n","    values = [float(x) for x in line.split(',')[1:]]\n","    float_data[i, :] = values\n","print(float_data.shape)   # 共有 420551 個時間點的天氣資料, 每個包含 14 種天氣數值"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PxpuKtx5qp_T"},"source":["#觀察整個年度的變化\n","from matplotlib import pyplot as plt\n","\n","temp = float_data[:, 1] # 索引 1 為 temperature 資料\n","plt.plot(range(len(temp)), temp)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vc_K2aX9qt41"},"source":["#觀察資料集中前10天的溫度資料\n","plt.plot(range(1440), temp[:1440])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GnNLfVz7qxcr"},"source":["#標準化資料\n","mean = float_data[:200000].mean(axis=0)\n","float_data -= mean\n","std = float_data[:200000].std(axis=0)\n","float_data /= std"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KHoRiwgOqzFa"},"source":["#定義產生器函式以產生時間序列樣本及其目標樣本\n","def generator(data,lookback,delay,min_index,max_index,shuffle=False,batch_size=128,step=6):\n","    if max_index is not None:\n","        max_index = len(data) - delay -1\n","    \n","    i = min_index + lookback\n","\n","    while 1:\n","        if shuffle:\n","            rows = np.random.randint(min_index+lookback , max_index ,size=batch_size)\n","        else:\n","            if i+batch_size >= max_index:\n","                i = min_index + lookback\n","            rows = np.arange(i,min(i+batch_size,max_index))\n","            i += len(rows)\n","            \n","        samples = np.zeros((len(rows),lookback//step,data.shape[-1]))\n","\n","        targets = np.zeros((len(rows),))\n","\n","        for j,row in enumerate(rows):\n","            indices = range(rows[j]-lookback,rows[j],step)\n","            samples[j]= data[indices]\n","            targets[j] = data[rows[j]+delay][1]\n","        \n","        yield samples,targets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T6NKKRQMq4QL"},"source":["#建立訓練資料、驗證資料與測試資料產生器\n","lookback = 1440\n","step = 6\n","delay = 144\n","batch_size = 128\n","\n","# 訓練資料產生器\n","train_gen = generator(float_data,lookback=lookback,delay=delay,min_index = 0,max_index=200000,shuffle=True,step=step,batch_size=batch_size)\n","\n","# 驗證資料產生器\n","val_gen = generator(float_data,lookback=lookback,delay=delay,min_index = 200001,max_index=300000,shuffle=True,step=step,batch_size=batch_size)\n","\n","# 測試資料產生器\n","val_gen = generator(float_data,lookback=lookback,delay=delay,min_index = 300001,max_index=None,shuffle=True,step=step,batch_size=batch_size)\n","\n","val_steps = (300000-200001-lookback) // batch_size\n","test_steps = (len(float_data)-300001-lookback) // batch_size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3EQHj3Tnq6Ci"},"source":["def evaluate_naive_method():\n","    batch_maes = []\n","    for step in range(val_steps):           # 計算所有驗證集資料\n","        samples , targets = next(val_gen)   # 驅動產生器\n","       # print(samples.shape)                # 回朔1440個時間點，並以6個時間點作為間隔進行取樣→共產生1440/6，讓他吐出一個批次輛\n","       # print(targets.shape)\n","        preds = samples[:,-1,1]\n","        mae = np.mean(np.abs(preds-targets))\n","        batch_maes.append(mae)\n","    print(np.mean(batch_maes))\n","\n","evaluate_naive_method()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ukN4GRTFq_ET"},"source":["celsius_mae = 0.29*std[1]\n","celsius_mae #得到攝氏溫度誤差2.57℃"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h87g1E-drAw5"},"source":[""],"execution_count":null,"outputs":[]}]}